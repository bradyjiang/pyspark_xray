# pyspark-xray
:toc:

last updated: May 31, 2020

pyspark_xray is a diagnostic tool, in the form of Python library, for pyspark developers to debug and troubleshoot PySpark applications locally, specifically it enables local debugging of PySpark RDD transformation functions.

The purpose of developing pyspark_xray is to create a development framework that enables PySpark application developers to debug and troubleshoot locally and do production runs remotely using the same code base.

## Problem

For developers, it's very important to do step-by-step debugging of every part of an application locally in order to diagnose, troubleshoot and solve problems during development.  Given the facts that Spark framework is designed to run on a cluster of computers rather than a single node, and part of Spark code runs on master node and remaining part runs on slave nodes, currently Spark developer community does not have a proper tool to locally run and debug Spark application code that runs on slave nodes.  Spark code that runs on slave nodes usually include lambda functions as parameters for https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations[RDD transformations].

## Solution

https://github.com/bradyjiang/pyspark_xray/tree/master/pyspark_xray[**pyspark_xray**] library enables developers to locally debug (step into) 100% of Spark application code, not only code that runs on master node, but also code that runs on slave nodes, using PyCharm and other popular IDE such as VSCode.  After integrating your PySpark application with pyspark_xray library, with the use of `CONST_BOOL_LOCAL_MODE` variable from https://github.com/bradyjiang/pyspark_xray/blob/master/pyspark_xray/const.py[const.py] in your Spark code base, you can locally debug and remotely execute your Spark application using the same code base.

check out <<demo-app, demo app>> section for an example use case.

# Demo App

The https://github.com/bradyjiang/pyspark_xray/tree/master/demo_app[**demo_app**] folder contains a simple PySpark application that takes data of 11 loans into a Spark RDD, then calculates interest amount for each loan by passing `calc_mthly_payment` and `calc_interest` lambda functions respectively to RDD `mapValues` transformation to calculate monthly payment amount and total interest amount for each loan respectively, see input and output below

```
ingested 11 loans
input =
+-------+--------+-----+----------+
|loan_id|loan_amt|  apr|term_years|
+-------+--------+-----+----------+
|    300| 15000.0|0.054|         6|
|    301| 27000.0|0.034|         6|
|    302| 33000.0|0.053|         5|
|    303| 45000.0|0.035|         5|
|    304| 56000.0|0.033|         7|
|    305| 44000.0|0.032|         4|
|    306| 25000.0|0.043|         5|
|    307| 26000.0|0.023|         7|
|    308| 35200.0|0.034|         6|
|    309| 57000.0|0.055|         5|
|    310| 45300.0|0.034|         5|
+-------+--------+-----+----------+

payment output =
+-------+---------------+
|loan_id|monthly_pmt_amt|
+-------+---------------+
|    300|         244.37|
|    301|         415.08|
|    302|          627.3|
|    303|         818.63|
|    304|         747.54|
|    305|          977.8|
|    306|         463.81|
|    307|          335.4|
|    308|         541.14|
|    309|        1088.77|
|    310|         822.06|
+-------+---------------+

interest output =
+-----------------+-------+
|     interest_amt|loan_id|
+-----------------+-------+
|           4860.0|    300|
|5508.000000000001|    301|
|           8745.0|    302|
|7875.000000000001|    303|
|          12936.0|    304|
|           5632.0|    305|
|           5375.0|    306|
|           4186.0|    307|
|7180.800000000001|    308|
|          15675.0|    309|
|           7701.0|    310|
+-----------------+-------+
```

## Dependencies

* pyspark_xray (this package)
* spark
* pyspark
* java
* PyCharm

## Preparation

* Open command line, kick off `java` command, if you get an error, then download and install **java** (version 1.8.0_221 as of April 2020)
* If you don't have it, download and install **PyCharm** Community edition (version 2020.1 as of April 2020)
* If you don't have it, download and install Anaconda Python 3.7 runtime
* Download and install **spark** latest Pre-built for Apache Hadoop (spark-2.4.5-bin-hadoop2.7 as of April 2020, 200+MB size) locally
  ** **Windows**:
    *** if you don't have unzip tool, please download and install 7zip, a free tool to zip/unzip files
    *** extract contents of spark tgz file to c:\spark-x.x.x-bin-hadoopx.x folder
    *** follow the steps in https://medium.com/big-data-engineering/how-to-install-apache-spark-2-x-in-your-pc-e2047246ffc3[this tutorial]
        **** install `winutils.exe` into `c:\spark-x.x.x-bin-hadoopx.x\bin` folder, without this executable, you will run into error when writing engine output
  ** **Mac**:
    *** extract contents of spark tgz file to \Users\[USERNAME]\spark-x.x.x-bin-hadoopx.x folder
* install **pyspark** by `pip install pyspark` or `conda install pyspark`

## Run Configuration

You run Spark application on a cluster from command line by issuing `spark-submit` command which submit a Spark job to the cluster.  But from PyCharm or other IDE on a local laptop or PC, `spark-submit` cannot be used to kick off a Spark job.  Instead, follow these steps to set up a Run Configuration of pyspark_xray's demo_app on PyCharm

* Set Environment Variables:
** set `HADOOP_HOME` value to `C:\spark-2.4.5-bin-hadoop2.7`
** set `SPARK_HOME` value to `C:\spark-2.4.5-bin-hadoop2.7`
* use Github Desktop or other git tools to clone `pyspark_xray` from Github
* PyCharm > Open pyspark_xray as project
* Open PyCharm > Run > Edit Configurations > Defaults > Python and enter the following values:
  ** **Environment variables** (Windows): `PYTHONUNBUFFERED=1;PYSPARK_PYTHON=python;PYTHONPATH=$SPARK_HOME/python;PYSPARK_SUBMIT_ARGS=pyspark-shell;`
* Open PyCharm > Run > Edit Configurations, create a new Python configuration, point the script to the path of `driver.py` of pyspark_xray > demo_app (see screen shot below)

image::https://github.com/bradyjiang/pyspark_xray/raw/master/docs/screen-shots/driver-run-config.png[driver run configuration]

## Debug Locally

In main.py, for demonstration purpose, RDD transformation functions are called at two places

* first calling native RDD mapValues function and `calc_mthly_interest` function like this `rdd_pmt = loan_rdd.mapValues(lambda x: utils_slave.calc_mthly_payment(row=x))`
* second calling pyspark_xray's wrapper of RDD mapValues `wrapper_mapvalues` and `calc_interest` function like this `utils_debugger.wrapper_mapvalues`

image::https://github.com/bradyjiang/pyspark_xray/raw/master/docs/screen-shots/stopped-main-wrapper-mapvalues.png[stopped at main wrapper mapvalues]

Corresponding to them, we set break points in  `calc_mthly_payment` and `calc_interest` lambda functions respectively in utils_s.py.  Among these 2 break points, only the 2nd one will be stopped as shown below, the 1st one will NEVER be stopped

image::https://github.com/bradyjiang/pyspark_xray/raw/master/docs/screen-shots/stopped-utils_s-calc-interest.png[stopped at calc_interest RDD transformation function]

# References

PySpark Resources:

* https://www.reddit.com/r/apachespark/[reddit r/apachespark]
* https://github.com/topics/pyspark[pyspark topic] on Github
* another pyspark tuning tool: https://github.com/msukmanowsky/drpyspark[drpyspark]
